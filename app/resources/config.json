{
    "reqid": "20190501204341",
    "models": {
        "Model1": {
            "modelPath": "snn",
            "modelType": "keras",
            "w2v": "yes",
            "handleType": "wordVectorsSum"
        }
    },
    "w2v": {
        "modelPath": "W2VModel.vec.pkl",
        "ndim": 100
    },
    "tokenization": {
        "actualtoks": "yes",
        "normalization": "yes",
        "stopwords": "yes",
        "expos": "PUNC,DT,IN,CD,PRP,RP,RB,W,PDT",
        "extrawords": "",
        "maxseqlen": 300,
        "maxcharsseqlen": 512,
        "rttaggerpath": "ArabicDocumentsTokenizer.jar"
    },
    "labels": "labels.txt"
}